{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing, model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "\n",
    "import mytrain_lib as ml\n",
    "\n",
    "import importlib\n",
    "\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train      = 'F://TFG//datasets//data_train//'\n",
    "path_graphs     = 'F://TFG//graphs//plot_results//'\n",
    "path_results    = 'F://TFG//results//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_train+'training_features_DF.csv',sep=';',index_col='wyId')\n",
    "raw_Data = pd.read_json('F://TFG//datasets/raw_datasets//RAW_partidos.json').set_index('wyId')\n",
    "\n",
    "# X_train = pd.read_csv(path_train+'X_train.csv',sep=';',index_col='wyId')\n",
    "# y_train = pd.read_csv(path_train+'y_train.csv',sep=';',index_col='wyId')\n",
    "# X_test = pd.read_csv(path_train+'X_test.csv',sep=';',index_col='wyId')\n",
    "# y_test = pd.read_csv(path_train+'y_test.csv',sep=';',index_col='wyId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = ml.FootballMatchesDataset(file = 'train')\n",
    "test_data   = ml.FootballMatchesDataset(file = 'test')\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=0)\n",
    "train_feat, train_lab, m = next(iter(dataloader))\n",
    "train_lab, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler  = preprocessing.StandardScaler()\n",
    "train_data.data = scaler.fit_transform(train_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation\n",
    "\n",
    "Define the class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I) Artificial Neural Network Approach to Football Score Prediction\n",
    "\n",
    "Multilayer Perceptron with 1 hidden layer with BacpPropagation.\n",
    "6 units input -> 5 hidden units -> 2 output units w/ sigmoid\n",
    "\n",
    "Data Normalized [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = ml.FootballMatchesDataset(file = 'train')\n",
    "test_data   = ml.FootballMatchesDataset(file = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalizer()\n",
    "train_data.data = normalizer.fit_transform(train_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.data.mean(), train_data.data.std())\n",
    "print(train_data.data.max(),  train_data.data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train    = DataLoader(train_data, batch_size=20, shuffle=True)\n",
    "dataloader_test     = DataLoader(test_data,  batch_size=20, shuffle=True)\n",
    "\n",
    "train_feat, train_lab, m = next(iter(dataloader_train))\n",
    "train_feat[:3],train_lab[:3] ,m[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_Data.loc[[2576042, 2500003, 2576265]].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_feature, ouput_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.h1 = nn.Linear(in_features=input_feature,out_features=5)\n",
    "        self.bn = nn.BatchNorm1d(5)\n",
    "        self.out = nn.Linear(5,ouput_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.h1(x))\n",
    "        x = self.bn(x)\n",
    "        return F.softmax(self.out(x),1)    \n",
    "\n",
    "    def reset_weights(self):\n",
    "        self.h1.reset_parameters()\n",
    "        self.bn.reset_parameters()\n",
    "        self.out.reset_parameters()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(22,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the architecture and number of parameters.\n",
    "print(model)\n",
    "print(f\"The model has {sum([x.nelement() for x in model.parameters()]):,} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function: Cross-entropy Loss\n",
    "\n",
    "we can provide `weights`, as prior probability of each class $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.labels   # in 1-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_class = np.mean(train_data.labels.numpy(),axis=0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizar con momentum (nesterov), weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = NeuralNetwork(22,3)\n",
    "epochs = 5\n",
    "learning_rate = 1e-1\n",
    "optimizer_lenet = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "error,accuracy_train,accuracy_test,confusion_matrix = ml.train_model(model, criterion, \n",
    "                                            optimizer, dataloader_train, dataloader_test, epochs)\n",
    "\n",
    "ml.save_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for p in [accuracy_train,accuracy_test,error]:\n",
    "    sn.lineplot(x=range(1,6),y=p)\n",
    "\n",
    "plt.title('Accuracy: MLP 5 hidden units, batch_size=20')\n",
    "plt.xticks(np.arange(epochs)+1)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim([0,1.5])\n",
    "plt.savefig(path_graphs + 'acc_mlp5_bn20_ej1.jpg', format='jpg', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "kfold = model_selection.KFold(n_splits=folds,shuffle=True,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.log = {}\n",
    "\n",
    "error, accuracy_train, accuracy_test, confusion_matrix = (ml.train_wCrossValidation(\n",
    "                                        NeuralNetwork(22,3),criterion, optimizer, \n",
    "                                        train_data, kfold, epochs=20))\n",
    "\n",
    "ml.save_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 4\n",
    "(ml.dispConfusionMatrix(confusion_matrix[f],\n",
    "        'Confusion matrix: Normaliz. MLP 1x5, bn=20',\n",
    "        'confmat_norm_mlp5_bn20_ej2_' + str(f) ,\n",
    "        save=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for p in error:\n",
    "    plt.plot(p)\n",
    "\n",
    "plt.title('Error Cross-Validation: MLP 5 hidden units, batch_size=20')\n",
    "plt.xticks(np.arange(20))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('error')\n",
    "plt.ylim([0.5,1.5])\n",
    "# plt.savefig(path_graphs + 'error_cv5_mlp5_bn20_ej2.jpg', format='jpg', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ml.FootballMatchesDataset(file = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparametros generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model       = NeuralNetwork(22,3)\n",
    "\n",
    "# scaling/normalization\n",
    "scalers = [None,preprocessing.MinMaxScaler(), preprocessing.Normalizer(), \n",
    "               preprocessing.StandardScaler()]\n",
    "\n",
    "# loss function\n",
    "weights_class = torch.tensor(np.mean(train_data.labels.numpy(),axis=0))\n",
    "criterions = [nn.CrossEntropyLoss()]\n",
    "\n",
    "# optimizer\n",
    "learning_rate = [0.5,0.1,1e-2]\n",
    "\n",
    "# cross-validation\n",
    "folds = 5\n",
    "kfold = model_selection.KFold(n_splits=folds,shuffle=True,random_state=0)\n",
    "\n",
    "# batch-size\n",
    "bs = [10,20,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(error,accuracy_train,accuracy_test,confusion_matrix):\n",
    "    confusion_matrix = np.array(confusion_matrix)\n",
    "    accuracy_test    = np.array(accuracy_test)\n",
    "    accuracy_train   = np.array(accuracy_train)\n",
    "    error            = np.array(error)\n",
    "\n",
    "    acc_test_lastepoch = accuracy_test[:,:,-1]  # only interested in last epoch\n",
    "\n",
    "    # best models of each configuration\n",
    "    best_cv          = acc_test_lastepoch.argmax(axis=1)\n",
    "    best_config_cv   = acc_test_lastepoch.argmax(axis=0)\n",
    "\n",
    "    # best configurations are:\n",
    "    print('config','\\t', 'accuracy_test\\t', '\\taccuracy_train\\t', '\\terror')\n",
    "\n",
    "    for c in best_config_cv:\n",
    "        print(c,'\\t', accuracy_test[c,best_cv[c],-1]\n",
    "                    , accuracy_train[c,best_cv[c],-1]\n",
    "                    , error[c,best_cv[c],-1])\n",
    "\n",
    "    hyperparams = (np.array(np.meshgrid(scalers,criterion,learning_rate,b1,b2\n",
    "                        ,weight_decay,bs)).T.reshape((-1,7)))\n",
    "\n",
    "    temp = datetime.now().strftime(\"_%m%d_%H%M%S\")\n",
    "\n",
    "    for i,c in enumerate(best_config_cv):\n",
    "        print(f'Config of {c} - Fold {best_cv[c]}: {hyperparams[c]}')\n",
    "        ml.dispConfusionMatrix(confusion_matrix[c,best_cv[c]],\n",
    "                        f'Confusion Matrix: MLP 1x5 SGD {hyperparams[c]}',\n",
    "                        f'confmat_mlp5_SGD_t{temp}_id{i}', save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = [0.01,0.1,0.9,0.99]\n",
    "nesterov = dampening = [True, False]\n",
    "\n",
    "# EXPERIMENTS with Stochastic Gradient Descend and 10 epochs.\n",
    "importlib.reload(ml)\n",
    "error,accuracy_train,accuracy_test,confusion_matrix = ml.Grid_Search_SGD(train_data,scalers,\n",
    "                                            criterions,learning_rate,momentum,model,\n",
    "                                            kfold,batch_size=bs,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(error,accuracy_train,accuracy_test,confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTS with Stochastic Gradient Descend and 10 epochs.\n",
    "error,accuracy_train,accuracy_test,confusion_matrix = ml.Grid_Search_SGD(train_data,scalers,\n",
    "                                            criterions,learning_rate,momentum,model,\n",
    "                                            kfold,batch_size=bs,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(error,accuracy_train,accuracy_test,confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer   = torch.optim.Adam\n",
    "\n",
    "# params Adam\n",
    "r = np.random.rand\n",
    "b1, b2          = [0.01,0.1,0.9], [0.01,0.1,0.9,0.99]\n",
    "weight_decay    = [0,1,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 epochs\n",
    "importlib.reload(ml)\n",
    "\n",
    "error,accuracy_train,accuracy_test,confusion_matrix = ml.Grid_Search_Adam(train_data,scalers,\n",
    "                                            criterions,learning_rate,b1,b2,model,\n",
    "                                            kfold,batch_size=bs,weight_decay=weight_decay,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(error,accuracy_train,accuracy_test,confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epochs\n",
    "error,accuracy_train,accuracy_test,confusion_matrix = ml.Grid_Search_Adam(train_data,scalers,\n",
    "                                            criterions,learning_rate,momentum,model,\n",
    "                                            kfold,batch_size=bs,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(error,accuracy_train,accuracy_test,confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGUIENTES PASOS:\n",
    "\n",
    "# 1. otros optimizadores y epochs\n",
    "# 2. otra arquitectura de red\n",
    "    # a. reducir params\n",
    "    # b. añadir hidden layers\n",
    "# 3. reducir dimensionalidad / seleccionar caract."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('tfg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "288ff6fe8157f43ba3e1fb4cfa0011490e9beb907b54ad0d71ae70a61946bdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
