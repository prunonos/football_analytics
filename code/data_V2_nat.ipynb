{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import difflib as dl\n",
    "import funcTest as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-samoa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir('F:\\\\TFG\\\\datasets\\\\nature-dataset')\n",
    "\n",
    "# loading the events data\n",
    "events={}\n",
    "nations = ['Italy','England','Germany','France','Spain','European_Championship','World_Cup']\n",
    "for nation in nations:\n",
    "    with open('./events/events_%s.json' %nation) as json_data:\n",
    "        events[nation] = json.load(json_data)\n",
    "        \n",
    "# loading the match data\n",
    "matches={}\n",
    "nations = ['Italy','England','Germany','France','Spain','European_Championship','World_Cup']\n",
    "for nation in nations:\n",
    "    with open('./matches/matches_%s.json' %nation) as json_data:\n",
    "        matches[nation] = json.load(json_data)\n",
    "\n",
    "# loading the players data\n",
    "players={}\n",
    "with open('./players.json') as json_data:\n",
    "    players = json.load(json_data)\n",
    "    \n",
    "# loading the teams data\n",
    "teams={}\n",
    "with open('./teams.json',encoding='utf-8') as json_data:\n",
    "    teams = json.load(json_data)\n",
    "\n",
    "# loading the competitions data\n",
    "competitions={}\n",
    "with open('./competitions.json') as json_data:\n",
    "    competitions = json.load(json_data)\n",
    "    \n",
    "# loading the playerank data\n",
    "playerank={}\n",
    "with open('./playerank.json') as json_data:\n",
    "    playerank = json.load(json_data)\n",
    "    \n",
    "# loading the referees data\n",
    "# referees={}\n",
    "# with open('./referees.json') as json_data:\n",
    "#     referees = json.load(json_data)\n",
    "\n",
    "# loading the coaches data\n",
    "coaches={}\n",
    "with open('./coaches.json') as json_data:\n",
    "    coaches = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d50dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a97145",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches['Spain'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-governor",
   "metadata": {},
   "source": [
    "<h3>Vamos a concatenar todos los partidos de las 5 ligas</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "partidos = []\n",
    "\n",
    "paises = ['Italy','England','Germany','France','Spain']\n",
    "\n",
    "for pais in paises:\n",
    "    for partido in matches[pais]:\n",
    "        partidos.append(partido)\n",
    "        \n",
    "partidos = np.array(partidos,dtype=dict)\n",
    "partidos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-light",
   "metadata": {},
   "source": [
    "<h3>Crear tabla de partidos y filtrar atributos</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-database",
   "metadata": {},
   "source": [
    "<h4>Primero, agrupamos los datos que deseamos para los datasets</h4>\n",
    "\n",
    "<b>Dataset 1</b>: será un dataset más simple simplemente con resultados  y IDs de equipos, competiciones, temporadas y nº de jornada\n",
    "\n",
    "    - ID Partido\n",
    "    - ID Competicion\n",
    "    - ID Temporada\n",
    "    - Jornada (roundID)\n",
    "    - IDs partidos para cada equipo -> se calcula\n",
    "    - Fecha (dateutc) -> Formato YYYY-MM-DD hh:mm:ss\n",
    "    - ID Equipos\n",
    "    - Nombre Equipos\n",
    "    - Goles Equipos\n",
    "    - ID Ganador o 0 si empate\n",
    "    - Formaciones Equipos (si tienen 1 sino 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(partidos)\n",
    "s = pd.Series(partidos[0])\n",
    "s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['a',\n",
    "       'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f2d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_serie = [pd.Series(p) for p in partidos]\n",
    "# print(match_serie)\n",
    "rawDB = pd.DataFrame(match_serie, columns=(match_serie[0]).index)\n",
    "\n",
    "columnNamesDB = ['wyId','competitionId','seasonId','roundId','venue','dateutc','winner'] # wyId = ID del partido en el DB\n",
    "rawDB = rawDB[columnNamesDB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e84c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-rendering",
   "metadata": {},
   "source": [
    "<b>Añadimos ahora los datos de cada equipo:</b>\n",
    "\n",
    "Para añadirlos hay que tener en cuenta que los datos de cada equipo se encuentran como un objeto JSON dentro del JSON del partido, es decir dentro del atributo <b>teamsData</b>\n",
    "\n",
    "Dentro del atributo teamsData encontramos dos JSON, uno para cada equipo. El índice de cada JSON es el Id del equipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "teamsData = [list(m['teamsData'].values()) for m in match_serie] # partido x equipo\n",
    "pd.Series(teamsData[0][0])['formation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-jerusalem",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "teamsData = [list(m['teamsData'].values()) for m in match_serie] # partido x equipo\n",
    "\n",
    "homeT = []\n",
    "awayT = []\n",
    "\n",
    "for m in teamsData:\n",
    "#     print(m[0]['teamId'])\n",
    "    if m[0]['side'] == 'home':\n",
    "        homeT.append(pd.Series(m[0]))\n",
    "        awayT.append(pd.Series(m[1]))\n",
    "    else:\n",
    "        homeT.append(pd.Series(m[1]))\n",
    "        awayT.append(pd.Series(m[0]))\n",
    "           \n",
    "# print(set([x['teamId'] for x in homeT]))\n",
    "\n",
    "# debemos cambiar el nombre de las columnas para que no se llamen igual las Home que las Away\n",
    "\n",
    "colH = [str(x) + '_home' for x in homeT[0].index]\n",
    "colA = [str(x) + '_away' for x in awayT[0].index]\n",
    "\n",
    "hData = pd.DataFrame(homeT, columns=homeT[0].index)\n",
    "aData = pd.DataFrame(awayT, columns=awayT[0].index)\n",
    "hData.columns = colH\n",
    "aData.columns = colA\n",
    "\n",
    "# Eliminamos los atributos que no nos interesan\n",
    "\n",
    "\n",
    "# Añadimos el nombre del equipo -> hay que buscarlo en el JSON de equipos\n",
    "\n",
    "# homeNames = [teams['wyId'==hT['teamId']]['name'] for hT in homeT]\n",
    "# awayNames = [teams['wyId'==aT['teamId']]['name'] for aT in awayT]\n",
    "# print(homeNames[500])\n",
    "\n",
    "# hData = hData.insert(len(hData.columns),)\n",
    "\n",
    "assert len(hData) == len(aData) == len(rawDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "hData[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDB = pd.concat([rawDB,hData,aData],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc47e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDB['formation_home'] = np.array(rawDB['formation_home'],dtype=dict)\n",
    "rawDB['formation_away'] = np.array(rawDB['formation_away'],dtype=dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a267168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawDB.to_csv('F:\\\\TFG\\\\datasets\\\\data_train\\\\rawDB.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDB['formation_home'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf73afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDB.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-progressive",
   "metadata": {},
   "source": [
    "Ahora deberemos seleccionar y arreglar los datos que queremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDB.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDB = rawDB[[\n",
    "    'wyId', 'competitionId', 'seasonId', 'roundId', 'dateutc', 'winner',\n",
    "    'teamId_home','score_home','scoreHT_home','teamId_away', 'score_away',\n",
    "    'scoreHT_away'\n",
    "     ]]\n",
    "\n",
    "assert len(rawDB) == 1826 == len(set(rawDB['wyId']))\n",
    "\n",
    "rawDB.loc[360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "local = []\n",
    "visit = []\n",
    "\n",
    "for row in rawDB['teamId_home']:\n",
    "    for t in teams:\n",
    "        local.append(t['name']) if t['wyId']==row else ''\n",
    "        \n",
    "for row in rawDB['teamId_away']:\n",
    "    for t in teams:\n",
    "        visit.append(t['name']) if t['wyId']==row else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(local) == len(visit) == 1826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-public",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sacar nombre de los equipos\n",
    "\n",
    "for t in  teams:\n",
    "    print(t['name']) if t['wyId']==3161 else ''\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-philadelphia",
   "metadata": {},
   "source": [
    "<h3>Datasets que vamos a hacer</h3>\n",
    "\n",
    "    - Cuotas con Victoria local (1) o otro resultado (0) -> cuotas_2\n",
    "    - Cuotas con Victoria local (-1) empate (0) o victoria visitante (1) -> cuotas_3\n",
    "    - Cuotas binaria + ultimos N partidos -> cuotas_racha_N\n",
    "    - Cuotas binaria + ultimos N partidos + goles N partidos -> cuotas_racha_goles_N\n",
    "    - Otras: \n",
    "        - teniendo en cuenta clasificaciones pasadas\n",
    "        - teniendo en cuenta formación\n",
    "        - teniendo en cuenta "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-preservation",
   "metadata": {},
   "source": [
    "<b>Añadimos al dataset global las cuotas</b>\n",
    "\n",
    "Habrá que identificar cada partido en ambas bases de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('F:\\\\TFG\\\\datasets\\\\football-data\\\\')\n",
    "dir_footdata = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-cartridge",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "      \n",
    "#LALIGA\n",
    "data_fd = {}\n",
    "data_fd['Esp'] = pd.read_csv(dir_footdata +'\\\\SP1_'+ '1718' +'.csv')\n",
    "       \n",
    "#PREMIER\n",
    "data_fd['Ing'] = pd.read_csv(dir_footdata +'\\\\E0_'+ '1718' +'.csv')\n",
    "\n",
    "#BUNDESLIGA\n",
    "data_fd['Ale'] = pd.read_csv(dir_footdata +'\\\\D1_'+ '1718' +'.csv')\n",
    "\n",
    "#LIGUE 1\n",
    "data_fd['Fra'] = pd.read_csv(dir_footdata +'\\\\F1_'+ '1718' +'.csv')\n",
    "\n",
    "#SERIE A\n",
    "data_fd['Ita'] = pd.read_csv(dir_footdata +'\\\\I1_'+ '1718' +'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67407e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_fd['Esp'].B365H[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abed86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fd['Esp'][data_fd['Esp'].columns[:25]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-capacity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdataDB = pd.concat([data_fd['Esp'],data_fd['Ing'],data_fd['Ale'],data_fd['Fra'],data_fd['Ita']],ignore_index=True)\n",
    "fdataDB[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdataDB.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25806f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdataDB.to_csv('F:\\\\TFG\\\\datasets\\\\data_train\\\\fdataDB.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdataDB = fdataDB[['Div','Date','HomeTeam','AwayTeam','FTHG','FTAG', 'FTR','HS',\n",
    "                   'AS','HST','AST','HC','AC','HF','AF','B365H', 'B365D', 'B365A']]\n",
    "fdataDB.loc[[0,100,400,700,1000,1400,1700]]\n",
    "\n",
    "assert len(fdataDB) == len(rawDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-cream",
   "metadata": {},
   "source": [
    "Hay que identificar los equipos para identificar los partidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdTeams = set(fdataDB['HomeTeam'])\n",
    "\n",
    "# equipos nature dataset\n",
    "\n",
    "naTeams = [t['name'] for t in teams if t['type']=='club']\n",
    "\n",
    "assert len(fdTeams)==len(naTeams)==98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "iguales = [t for t in fdTeams if t in naTeams]\n",
    "len(iguales) # tenemos que 58 clubs se llaman igual en ambos datasets\n",
    "\n",
    "desiguales = [t for t in naTeams if t not in iguales]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iguales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "encontrados = []\n",
    "no_encontrados = []\n",
    "\n",
    "for des in desiguales:\n",
    "    find = dl.get_close_matches(des,fdTeams)\n",
    "    if len(find)>0:\n",
    "        encontrados.append((des,find[0]))\n",
    "    else:\n",
    "        no_encontrados.append(des)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encontrados)\n",
    "encontrados # (na, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "emparejadosFD = iguales\n",
    "emparejadosNA = iguales\n",
    "\n",
    "for (x,y) in encontrados:\n",
    "    emparejadosFD.append(y)\n",
    "    emparejadosNA.append(x)\n",
    "    \n",
    "desigualesFD = [t for t in fdTeams if t not in emparejadosFD]\n",
    "desigualesNA = [t for t in naTeams if t not in emparejadosNA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(desigualesFD) == 12 == len(desigualesNA)\n",
    "\n",
    "desigualesNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-category",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desigualesFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "encontrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_equipos = ['Celta','Alaves','Ath Bilbao','Lyon','Paris SG','Spal','Brighton','Inter',\n",
    "               'St Etienne','FC Koln','La Coruna','Malaga']\n",
    "\n",
    "equipos = {} # { nature_data : football_data } }\n",
    "\n",
    "for t in iguales:\n",
    "    equipos[t] = t\n",
    "\n",
    "for x,y in encontrados:\n",
    "    equipos[x] = y\n",
    "    \n",
    "for x,y in zip(desigualesNA,aux_equipos):\n",
    "    equipos[x] = y\n",
    "    \n",
    "# bug: los equipos 'encontrados' salen dupplicados?\n",
    "for _,y in encontrados:\n",
    "    if y in equipos.keys():\n",
    "        del(equipos[y]) \n",
    "        \n",
    "assert len(equipos) == 98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-inspiration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comprobamos que todos los elementos coinciden\n",
    "\n",
    "x = 0\n",
    "\n",
    "for e in equipos.keys():\n",
    "    x = x + naTeams.count(e)\n",
    "\n",
    "assert x == len(naTeams) == len(equipos.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f668d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "equipos['Deportivo La Coru\\\\u00f1a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-legislation",
   "metadata": {},
   "source": [
    "Ahora debemos encontrar asociar los equipos de football_data con los de nature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_ID_Names = {}\n",
    "\n",
    "for t in teams:\n",
    "    if t['type'] == 'club':\n",
    "        nat_ID_Names[t['wyId']] = t['name']\n",
    "        \n",
    "assert len(nat_ID_Names.keys()) == 98 == len(equipos.keys())\n",
    "\n",
    "aux = equipos\n",
    "equipos = {}\n",
    "\n",
    "equipos['equipos'] = aux\n",
    "equipos['nat_ID_Names'] = nat_ID_Names\n",
    "\n",
    "with open('F:\\\\TFG\\\\datasets\\\\data_train\\\\equipos.json','w') as js:\n",
    "    json.dump(equipos,js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79cb357",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-alabama",
   "metadata": {},
   "source": [
    "<b> Vamos a añadir ya las cuotas al dataset de Nature que hemos creado anteriormente </b>\n",
    "\n",
    "Para ello debemos iterar sobre el dataframe de Nature y buscar en el dataset de Football-Data que partido coincide con esos rivales.\n",
    "\n",
    "(Hay que tener en cuenta que si tuviesemos más de temporadas se repetiria un mismo enfrentamiento entre dos equipos distintos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-cinema",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rawDB[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdataDB[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rawDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a6452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-prince",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tarda mucho en ejecutarse\n",
    "\n",
    "data_cuotas = ['HS','AS','HST','AST','HC','AC','HF','AF','B365H', 'B365D', 'B365A']\n",
    "\n",
    "cuotaNatDB = rawDB\n",
    "cuotasFeat = {}\n",
    "\n",
    "for dc in data_cuotas:\n",
    "    cuotasFeat[dc] = np.empty(shape=(len(rawDB)),dtype=object)\n",
    "    \n",
    "index = 0\n",
    "\n",
    "for h,a in zip(rawDB['teamId_home'],rawDB['teamId_away']):\n",
    "    for i in range(len(rawDB)):\n",
    "        p = fdataDB.loc[i]\n",
    "        h_name = equipos[nat_ID_Names[h]]\n",
    "        a_name = equipos[nat_ID_Names[a]]\n",
    "        if p['HomeTeam']==h_name and p['AwayTeam']==a_name:\n",
    "            for dc in data_cuotas:\n",
    "                cuotasFeat[dc].put(index,p[dc])\n",
    "            index += 1\n",
    "            break\n",
    "\n",
    "for dc in data_cuotas:\n",
    "    assert len(cuotasFeat[dc]) == len(rawDB)\n",
    "    cuotaNatDB[dc] = cuotasFeat[dc]\n",
    "\n",
    "# # Tarda mucho en ejecutarse\n",
    "\n",
    "# bet_h_nat = []\n",
    "# bet_a_nat = []\n",
    "\n",
    "# for h,a in zip(rawDB['teamId_home'],rawDB['teamId_away']):\n",
    "#     for i in range(1826):\n",
    "#         p = fdataDB.loc[i]\n",
    "#         h_name = equipos[nat_ID_Names[h]]\n",
    "#         a_name = equipos[nat_ID_Names[a]]\n",
    "#         if p['HomeTeam']==h_name and p['AwayTeam']==a_name:\n",
    "#             bet_h_nat.append(p['B365H'])\n",
    "#             bet_a_nat.append(p['B365A'])\n",
    "#             break\n",
    "\n",
    "# assert len(bet_h_nat) == len(bet_a_nat) == 1826\n",
    "\n",
    "# cuotaNatDB = rawDB\n",
    "# cuotaNatDB['B365H'] = bet_h_nat\n",
    "# cuotaNatDB['B365A'] = bet_a_nat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORTAMOS CSV\n",
    "# cuotaNatDB.to_csv('F:\\\\TFG\\\\datasets\\\\data_train\\\\cuotaNatDB.csv',index=False)\n",
    "# cuotaNatDB.to_csv('F:\\\\TFG\\\\datasets\\\\data_train\\\\Nature_wStatsDB_v1.csv',index=False)\n",
    "\n",
    "\n",
    "# IMPORTAMOS CSV\n",
    "cuotaNatDB = pd.read_csv('F:\\\\TFG\\\\datasets\\\\data_train\\\\cuotaNatDB.csv')\n",
    "statsNatDB_v1 = pd.read_csv('F:\\\\TFG\\\\datasets\\\\data_train\\\\Nature_wStatsDB_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53edd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "statsNatDB_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-tenant",
   "metadata": {},
   "source": [
    "Creamos la base de datos para que la puede usar el Modelo\n",
    "\n",
    "- -1 gana local\n",
    "- 0 empate\n",
    "- 1 gana visitante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuotaNatDB = pd.read_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotaNatDB.csv')\n",
    "cuotaNatDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05261c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cuotas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-iraqi",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cuotasDB = cuotaNatDB[data_cuotas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcRES(db):\n",
    "    res = []\n",
    "    for h,a in zip(db['score_home'],db['score_away']):\n",
    "        if h>a:\n",
    "            res.append(-1)\n",
    "        elif a>h:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(0) \n",
    "    \n",
    "    assert len(res) == len(db)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LA EXPORTAMOS\n",
    "# cuotasDB.to_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotas3DB.csv',index=False)\n",
    "# cuotaNatDB.to_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotaNatDB.csv',index=False)\n",
    "\n",
    "# LO IMPORTAMOS\n",
    "cuotaNatDB = pd.read_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotaNatDB.csv')\n",
    "cuotasDB = pd.read_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotas3DB.csv')\n",
    "cuotasDB[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-affair",
   "metadata": {},
   "source": [
    "<b> Para el siguiente Dataset vamos a registrar la racha (resultados y goles) de los ultimos N partidos de un equipos </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-southwest",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cuotaNatDB.loc[cuotaNatDB['competitionId'] == 524][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuotaNatDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upt_features(feat,pts,hs_,goals,hst_,hc_,hf_,i):\n",
    "    feat['points'] += pts\n",
    "    feat['goals'] += goals\n",
    "    feat['shots'] += hs_\n",
    "    feat['on_target'] += hst_\n",
    "    feat['corners'] += hc_\n",
    "    feat['fouls'] += hf_\n",
    "    return i+1\n",
    "    \n",
    "def descartar(feat,i,n):\n",
    "    print(i, feat['goals'])\n",
    "    if i < n:\n",
    "        print('Partidos nulos')\n",
    "        feat['points'] = np.NaN\n",
    "        feat['goals'] = np.NaN\n",
    "        feat['shots'] = np.NaN\n",
    "        feat['on_target'] = np.NaN\n",
    "        feat['corners'] = np.NaN\n",
    "        feat['fouls'] = np.NaN    \n",
    "    \n",
    "def crear_tendencias(n=5,solo_local=True):\n",
    "    \n",
    "    features = {'ptsH': [], 'ptsA' : [], 'goalsH' : [], 'goalsA' : [],\n",
    "                'HS' : [], 'AS' : [], 'HST' : [], 'AST' : [], 'HC' : [], 'AC' : [], 'HF' : [], 'AF' : []}\n",
    "    id_comp = list(set(statsNatDB_v1['competitionId']))\n",
    "    aux_df = statsNatDB_v1\n",
    "\n",
    "    for comp in id_comp:\n",
    "        rows = statsNatDB_v1.index[statsNatDB_v1['competitionId']==comp]\n",
    "        df_comp = aux_df[rows[0]:rows[-1]+1]\n",
    "\n",
    "\n",
    "        for index in range(rows[0],rows[-1]+1):\n",
    "    #         print(index)\n",
    "            ht = df_comp['teamId_home'][index]\n",
    "            at = df_comp['teamId_away'][index]\n",
    "            len_H = 0\n",
    "            len_A = 0\n",
    "            \n",
    "            featH = {'points' : 0, 'goals' : 0, 'shots' : 0, 'on_target' : 0, 'corners' : 0, 'fouls' : 0}\n",
    "            featA = {'points' : 0, 'goals' : 0, 'shots' : 0, 'on_target' : 0, 'corners' : 0, 'fouls' : 0}\n",
    "\n",
    "            for i in range(index+1,rows[-1]+1):\n",
    "                p = df_comp.loc[i]  \n",
    "                h_aux = p['teamId_home']\n",
    "                if ht == h_aux:\n",
    "                    if p['score_home'] == p['score_away']:\n",
    "                        len_H = upt_features(featH,1,p['HS'],p['score_home'],p['HST'],p['HC'],p['HF'],len_H)\n",
    "                    elif p['score_home'] > p['score_away']:\n",
    "                        len_H = upt_features(featH,3,p['HS'],p['score_home'],p['HST'],p['HC'],p['HF'],len_H)\n",
    "                    else:\n",
    "                        len_H = upt_features(featH,0,p['HS'],p['score_home'],p['HST'],p['HC'],p['HF'],len_H)\n",
    "                elif ht == p['teamId_away'] and solo_local:\n",
    "                    if p['score_home'] == p['score_away']:\n",
    "                        len_H = upt_features(featH,1,p['AS'],p['score_away'],p['AST'],p['AC'],p['AF'],len_H)\n",
    "                    elif p['score_home'] > p['score_away']:\n",
    "                        len_H = upt_features(featH,0,p['AS'],p['score_away'],p['AST'],p['AC'],p['AF'],len_H)\n",
    "                    else:\n",
    "                        len_H = upt_features(featH,3,p['AS'],p['score_away'],p['AST'],p['AC'],p['AF'],len_H)\n",
    "\n",
    "                if len_H == n:\n",
    "                    break\n",
    "                \n",
    "            descartar(featH,len_H,n)\n",
    "\n",
    "            for i in range(index+1,rows[-1]+1):\n",
    "                p = df_comp.loc[i]\n",
    "                a_aux = p['teamId_home']\n",
    "                if at == a_aux and solo_local:\n",
    "                    if p['score_home'] == p['score_away']:\n",
    "                        len_A = upt_features(featA,1,p['HS'],p['score_home'],p['HST'],p['HC'],p['HF'],len_A)\n",
    "                    elif p['score_home'] > p['score_away']:\n",
    "                        len_A = upt_features(featA,3,p['HS'],p['score_home'],p['HST'],p['HC'],p['HF'],len_A)\n",
    "                    else:\n",
    "                        len_A = upt_features(featA,0,p['HS'],p['score_home'],p['HST'],p['HC'],p['HF'],len_A)\n",
    "                elif at == p['teamId_away']:\n",
    "                    if p['score_home'] == p['score_away']:\n",
    "                        len_A = upt_features(featA,1,p['AS'],p['score_away'],p['AST'],p['AC'],p['AF'],len_A)\n",
    "                    elif p['score_home'] > p['score_away']:\n",
    "                        len_A = upt_features(featA,0,p['AS'],p['score_away'],p['AST'],p['AC'],p['AF'],len_A)\n",
    "                    else:\n",
    "                        len_A = upt_features(featA,3,p['AS'],p['score_away'],p['AST'],p['AC'],p['AF'],len_A)\n",
    "\n",
    "                if len_A == n:\n",
    "                    break\n",
    "\n",
    "            descartar(featA,len_A,n)\n",
    "        \n",
    "            print()\n",
    "            \n",
    "            features['ptsH'].append(featH['points'])\n",
    "            features['goalsH'].append(featH['goals'])\n",
    "            features['HS'].append(featH['shots'])\n",
    "            features['HST'].append(featH['on_target'])\n",
    "            features['HC'].append(featH['corners'])\n",
    "            features['HF'].append(featH['fouls'])\n",
    "\n",
    "            features['ptsA'].append(featA['points'])\n",
    "            features['goalsA'].append(featA['goals'])\n",
    "            features['AS'].append(featA['shots'])\n",
    "            features['AST'].append(featA['on_target'])\n",
    "            features['AC'].append(featA['corners'])\n",
    "            features['AF'].append(featA['fouls'])\n",
    "                \n",
    "    assert len(features['ptsH']) == len(features['ptsA']) == len(cuotaNatDB)                    \n",
    "                \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = crear_tendencias(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "claves = []\n",
    "\n",
    "for c in list(features.keys()):\n",
    "    claves.append('tend_' + str(c))\n",
    "    \n",
    "claves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4bb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "tend_values = list(features.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dba637",
   "metadata": {},
   "outputs": [],
   "source": [
    "statsDB_tend3 = statsNatDB_v1\n",
    "\n",
    "for i in range(0,len(tend_values)):\n",
    "    statsDB_tend3[claves[i]] = tend_values[i]\n",
    "    \n",
    "statsDB_tend3['tend_AC']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f895bf",
   "metadata": {},
   "source": [
    "Vamos a simplificar el dataset para poder ser usado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de70ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calcRES(statsDB_tend3)\n",
    "statsDB_tend3['res'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc4305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LA EXPORTAMOS\n",
    "# cuotasDB_tend5.to_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotas3DB_tend5.csv',index=False)\n",
    "# cuotaNatDB.to_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotaNatDB.csv',index=False)\n",
    "# statsDB_tend3.to_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\statsDB_tend3.csv',index=False)\n",
    "\n",
    "# LO IMPORTAMOS\n",
    "cuotaNatDB = pd.read_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotaNatDB.csv')\n",
    "cuotasDB_tend5 = pd.read_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotas3DB_tend5.csv')\n",
    "statsDB_tend3 = pd.read_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\statsDB_tend3.csv')\n",
    "statsDB_tend3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95726c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cuotasDB_tend5[:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ec576",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuotasDB_tend3 = cuotasDB\n",
    "tendenciasH, tendenciasA = crear_tendencias(n=3)\n",
    "cuotasDB_tend3['tendencia_H'] = tendenciasH\n",
    "cuotasDB_tend3['tendencia_A'] = tendenciasA\n",
    "\n",
    "cuotasDB_tend3.to_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotasDB_tend3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf77d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cuotasDB_tend3\n",
    "dif_tend = [y-x for x,y in zip(d['tendencia_H'],d['tendencia_A'])]\n",
    "assert len(dif_tend) == len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuotasDB_tend3_norm = cuotasDB_tend3\n",
    "cuotasDB_tend3_norm['dif_tend'] = dif_tend\n",
    "cuotasDB_tend3_norm = ft.normalDB(cuotasDB_tend3_norm)\n",
    "cuotasDB_tend3_norm = cuotasDB_tend3_norm.drop(['tendencia_H', 'tendencia_A'], axis=1)\n",
    "cuotasDB_tend3_norm.to_csv('F:\\\\2020-21\\\\Beca VRAIN\\\\datasets\\\\data_train\\\\cuotasDB_tend3_norm.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = statsDB_tend3\n",
    "data = data.dropna(axis=0)\n",
    "data\n",
    "\n",
    "dif_puntos = data['tend_ptsA'] - data['tend_ptsH']\n",
    "dif_goles = data['tend_goalsA'] - data['tend_goalsH']\n",
    "dif_shots = data['tend_AS'] - data['tend_HS']\n",
    "dif_targets = data['tend_AST'] - data['tend_HST']\n",
    "dif_corners = data['tend_AC'] - data['tend_HC']\n",
    "\n",
    "data['dif_puntos'] = dif_puntos\n",
    "data['dif_goles'] = dif_goles\n",
    "data['dif_shots'] = dif_shots\n",
    "data['dif_targets'] = dif_targets\n",
    "data['dif_corners'] = dif_corners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-pound",
   "metadata": {},
   "source": [
    "Lo siguiente...\n",
    "\n",
    "Intentar visualizar los datos y probar algun modelo con Scikit Learn\n",
    "\n",
    "Quiero ver si hay alguna correlación entre ambas variables (cuotas, tendencias...), ya que de haberla, no tendria sentido usar ambas variables en el modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('tfg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "288ff6fe8157f43ba3e1fb4cfa0011490e9beb907b54ad0d71ae70a61946bdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
