{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing, model_selection, feature_selection\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline\n",
    "\n",
    "import mytrain_lib as ml\n",
    "\n",
    "import importlib\n",
    "\n",
    "torch.manual_seed(0)\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rawdata    = 'F://TFG//datasets/raw_datasets//'\n",
    "path_train      = 'F://TFG//datasets//data_train//'\n",
    "path_graphs     = 'F://TFG//graphs//'\n",
    "path            = 'F:/TFG/datasets/nature-dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_train+'training_features_DF.csv',sep=';',index_col='wyId')\n",
    "raw_Data = pd.read_json('F://TFG//datasets/raw_datasets//RAW_partidos.json').set_index('wyId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[2499738]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = ml.FootballMatchesDataset(file = 'train')\n",
    "test_data   = ml.FootballMatchesDataset(file = 'test')\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=0)\n",
    "train_feat, train_lab, m = next(iter(dataloader))\n",
    "train_lab, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler  = preprocessing.StandardScaler()\n",
    "train_data.data = scaler.fit_transform(train_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Implementation\n",
    "\n",
    "Define the class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I) Artificial Neural Network Approach to Football Score Prediction\n",
    "\n",
    "Multilayer Perceptron with 1 hidden layer with BacpPropagation.\n",
    "6 units input -> 5 hidden units -> 2 output units w/ sigmoid\n",
    "\n",
    "Data Normalized [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = ml.FootballMatchesDataset(file = 'train')\n",
    "test_data   = ml.FootballMatchesDataset(file = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalizer()\n",
    "train_data.data = normalizer.fit_transform(train_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.data.mean(), train_data.data.std())\n",
    "print(train_data.data.max(),  train_data.data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train    = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "dataloader_test     = DataLoader(test_data,  batch_size=128, shuffle=True)\n",
    "\n",
    "train_feat, train_lab, m = next(iter(dataloader_train))\n",
    "train_feat[:3],train_lab[:3] ,m[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_Data.loc[[2565618, 2501059, 2576040]].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_feature, ouput_classes, hidden_neurons=[5]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input = nn.Linear(in_features=input_feature, out_features=hidden_neurons[0])\n",
    "        self.hidden_layers, self.hidden_bn = nn.ModuleList([]),nn.ModuleList([])\n",
    "        for i in range(1,len(hidden_neurons)):\n",
    "            self.hidden_bn.append(nn.BatchNorm1d(hidden_neurons[i-1]))\n",
    "            self.hidden_layers.append(nn.Linear(in_features=hidden_neurons[i-1],out_features=hidden_neurons[i]))\n",
    "        \n",
    "        self.bn_out = nn.BatchNorm1d(hidden_neurons[-1])\n",
    "        self.out = nn.Linear(hidden_neurons[-1],ouput_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.input(x)\n",
    "        for bn,layer in zip(self.hidden_bn,self.hidden_layers):\n",
    "            x = layer(F.relu(bn(x)))\n",
    "        x = F.relu(self.bn(x))\n",
    "        return F.softmax(self.out(x),1)    \n",
    "\n",
    "    def reset_weights(self):\n",
    "        self.input.reset_parameters()\n",
    "        for bn,layer in zip(self.hidden_bn,self.hidden_layers):\n",
    "            bn.reset_parameters(); layer.reset_parameters()\n",
    "        self.bn.reset_parameters()\n",
    "        self.out.reset_parameters()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (input): Linear(in_features=160, out_features=20, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  )\n",
      "  (hidden_bn): ModuleList(\n",
      "    (0): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (bn_out): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (out): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n",
      "The model has 3,573 parameters.\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(160,3,hidden_neurons=[20,10,5])\n",
    "# Print out the architecture and number of parameters.\n",
    "print(model)\n",
    "print(f\"The model has {sum([x.nelement() for x in model.parameters()]):,} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function: Cross-entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can provide `weights`, as prior probability of each class $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.labels   # in 1-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_class = np.mean(train_data.labels.numpy(),axis=0)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([[0.15,0.24,0.61],[0.18,0.59,0.23],[0.35,0.34,0.31]]).float()\n",
    "target = torch.tensor([[0,1,0],[0,1,0],[1,0,0]]).float()\n",
    "print(F.softmax(input),target)\n",
    "\n",
    "output = nn.CrossEntropyLoss()(input, target)\n",
    "print(output.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizar con momentum (nesterov), weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "importlib.reload(ml)\n",
    "\n",
    "ml.log = {}\n",
    "\n",
    "model = NeuralNetwork(22,3)\n",
    "epochs = 100\n",
    "learning_rate = 1e-1\n",
    "optimizer_lenet = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "error,accuracy_train,accuracy_test,confusion_matrix = ml.train_model(model, criterion, \n",
    "                                            optimizer, dataloader_train, dataloader_test, epochs)\n",
    "\n",
    "ml.save_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for p in [accuracy_train,accuracy_test,error]:\n",
    "    sn.lineplot(x=range(1,6),y=p)\n",
    "\n",
    "plt.title('Accuracy: MLP 5 hidden units, batch_size=20')\n",
    "plt.xticks(np.arange(epochs)+1)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim([0,1.5])\n",
    "plt.savefig(path_graphs + 'acc_mlp5_bn20_ej1.jpg', format='jpg', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "kfold = model_selection.KFold(n_splits=folds,shuffle=True,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.log = {}\n",
    "\n",
    "model = NeuralNetwork(22,3)\n",
    "\n",
    "error, accuracy_train, accuracy_test, confusion_matrix = (ml.train_wCrossValidation(\n",
    "                                        NeuralNetwork(22,3),criterion, optimizer, \n",
    "                                        train_data, kfold, epochs=100))\n",
    "\n",
    "temp = datetime.now().strftime(\"_%m%d_%H%M%S\")\n",
    "ml.save_logging(temp, title='debug_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 4\n",
    "(ml.dispConfusionMatrix(confusion_matrix[f],\n",
    "        'Confusion matrix: Normaliz. MLP 1x3, bn=20',\n",
    "        'confmat_norm_mlp5_bn20_ej2_' + str(f) ,\n",
    "        save=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "for f,p in enumerate(error):\n",
    "    plt.plot(p,label=f'{f}')\n",
    "\n",
    "plt.title('Error Cross-Validation: MLP 5 hidden units, batch_size=20')\n",
    "plt.xticks(np.arange(20))\n",
    "plt.legend(title='Folder')\n",
    "# plt.grid()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('error')\n",
    "plt.ylim([np.min(error)-0.3,np.max(error)+0.3])\n",
    "# plt.savefig(path_graphs + 'error_cv5_mlp5_bn20_ej2.jpg', format='jpg', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(adam=False, anova=[5, 10], betas=[[0.01, 0.1, 0.5], [0.001, 0.01]], dataset='wyscout', drop=[], feat=None, lr=[0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10], pca=None, scaler=['minmax', 'std'], sgd=True, units=[2, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('dataset', type=str)\n",
    "parser.add_argument('drop', type=str, nargs='*')\n",
    "group_opt = parser.add_mutually_exclusive_group()\n",
    "group_opt.add_argument(\"-sgd\", action=\"store_true\")\n",
    "group_opt.add_argument(\"-adam\", action=\"store_true\")\n",
    "parser.add_argument(\"-betas\", nargs='+', type=list, default=[[.01, .1, .5],[.001, .01]])\n",
    "group_dimred = parser.add_mutually_exclusive_group()\n",
    "group_dimred.add_argument(\"-anova\", nargs='+', type=int)\n",
    "group_dimred.add_argument(\"-pca\", nargs='+', type=int)\n",
    "group_dimred.add_argument(\"-feat\", nargs='+', type=int)\n",
    "parser.add_argument('-units', type=int, nargs='+')\n",
    "parser.add_argument('-scaler', type=str, choices=['minmax','norm','std','maxabs'], nargs='+' )\n",
    "parser.add_argument('-lr' ,type=float, nargs='+', default=[.0001,.001,.01,.1,.5,1,10])\n",
    "args = parser.parse_args('wyscout -sgd -anova 5 10 -units 2 5 10 -scaler minmax std'.split())\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ml.FootballMatchesDataset(file = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiperparametros generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model       = NeuralNetwork(22,3)\n",
    "\n",
    "# scaling/normalization\n",
    "scalers = [None,preprocessing.MinMaxScaler(), preprocessing.Normalizer()]\n",
    "\n",
    "# loss function\n",
    "weights_class = torch.tensor(np.mean(train_data.labels.numpy(),axis=0))\n",
    "criterions = [nn.CrossEntropyLoss(), nn.CrossEntropyLoss(weight=weights_class)]\n",
    "\n",
    "# optimizer\n",
    "learning_rate = [0.5,0.1,1e-2]\n",
    "\n",
    "# cross-validation\n",
    "folds = 5\n",
    "kfold = model_selection.KFold(n_splits=folds,shuffle=True,random_state=0)\n",
    "\n",
    "# batch-size\n",
    "bs = [32,64,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(error,accuracy_train,accuracy_test,confusion_matrix, hyperparams):\n",
    "    confusion_matrix = np.array(confusion_matrix)\n",
    "    accuracy_test    = np.array(accuracy_test)\n",
    "    accuracy_train   = np.array(accuracy_train)\n",
    "    error            = np.array(error)\n",
    "\n",
    "    acc_test_lastepoch = accuracy_test[:,:,-1]  # only interested in last epoch\n",
    "\n",
    "    # best models of each configuration\n",
    "    best_cv          = acc_test_lastepoch.argmax(axis=1)\n",
    "    best_config_cv   = np.unique(acc_test_lastepoch.argmax(axis=0))\n",
    "\n",
    "    # best configurations are:\n",
    "    print('config','\\t', 'accuracy_test\\t', '\\taccuracy_train\\t', '\\terror')\n",
    "\n",
    "    for c in best_config_cv:\n",
    "        print(c,'\\t', accuracy_test[c,best_cv[c],-1]\n",
    "                    , accuracy_train[c,best_cv[c],-1]\n",
    "                    , error[c,best_cv[c],-1])\n",
    "\n",
    "    temp = datetime.now().strftime(\"_%m%d_%H%M%S\")\n",
    "\n",
    "    for i,c in enumerate(best_config_cv):\n",
    "        print(f'Config of {c} - Fold {best_cv[c]}: {hyperparams[c]}')\n",
    "        ml.dispConfusionMatrix(confusion_matrix[c,best_cv[c]],\n",
    "                        f'Confusion Matrix: MLP 1x5 SGD {hyperparams[c]}',\n",
    "                        f'confmat_mlp5_SGD_t{temp}_id{i}', save=True)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic Gradient Descend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = [0.01,0.9]\n",
    "nesterov = dampening = [True, False]\n",
    "\n",
    "# EXPERIMENTS with Stochastic Gradient Descend and 10 epochs.\n",
    "importlib.reload(ml)\n",
    "error,accuracy_train,accuracy_test,confusion_matrix = ml.Grid_Search_SGD(train_data,scalers,\n",
    "                                            criterions,learning_rate,momentum,model,\n",
    "                                            kfold,batch_size=bs,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(error,accuracy_train,accuracy_test,confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTS with Stochastic Gradient Descend and 10 epochs.\n",
    "error,accuracy_train,accuracy_test,confusion_matrix = ml.Grid_Search_SGD(train_data,scalers,\n",
    "                                            criterions,learning_rate,momentum,model,\n",
    "                                            kfold,batch_size=bs,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(error,accuracy_train,accuracy_test,confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer   = torch.optim.Adam\n",
    "\n",
    "# params Adam\n",
    "r = np.random.rand\n",
    "b1, b2          = [0.01,0.1,0.9], [0.01,0.1,0.9,0.99]\n",
    "weight_decay    = [0,1,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 epochs\n",
    "importlib.reload(ml)\n",
    "\n",
    "error,accuracy_train,accuracy_test,confusion_matrix = ml.Grid_Search_Adam(train_data,scalers,\n",
    "                                            criterions,learning_rate,b1,b2,model,\n",
    "                                            kfold,batch_size=bs,weight_decay=weight_decay,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(error,accuracy_train,accuracy_test,confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 epochs\n",
    "error,accuracy_train,accuracy_test,confusion_matrix = ml.Grid_Search_Adam(train_data,scalers,\n",
    "                                            criterions,learning_rate,momentum,model,\n",
    "                                            kfold,batch_size=bs,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = (np.array(np.meshgrid(scalers,criterions,learning_rate,b1,b2\n",
    "                        ,weight_decay,bs)).T.reshape((-1,7)))\n",
    "\n",
    "plot_results(error,accuracy_train,accuracy_test,confusion_matrix,hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction: PCA\n",
    "\n",
    "train_data = ml.FootballMatchesDataset('train')\n",
    "print(train_data.data.shape)\n",
    "\n",
    "pca = PCA(n_components=10,random_state=0)\n",
    "train_data.data = torch.tensor(pca.fit_transform(train_data.data))\n",
    "print(train_data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling/normalization\n",
    "scalers = [None,preprocessing.MinMaxScaler(), preprocessing.Normalizer()]\n",
    "\n",
    "# loss function\n",
    "weights_class = torch.tensor(np.mean(train_data.labels.numpy(),axis=0))\n",
    "criterions = [nn.CrossEntropyLoss(), nn.CrossEntropyLoss(weight=weights_class)]\n",
    "\n",
    "# optimizer\n",
    "learning_rate = [0.1,0.5]\n",
    "\n",
    "# cross-validation\n",
    "folds = 5\n",
    "kfold = model_selection.KFold(n_splits=folds,shuffle=True,random_state=0)\n",
    "\n",
    "# batch-size\n",
    "bs = [10,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params SGD\n",
    "momentum = [0.1,0.9]\n",
    "nesterov = dampening = [True]\n",
    "\n",
    "# params Adam\n",
    "b1, b2          = [0.01,0.1,0.9], [0.01,0.1,0.9,0.99]\n",
    "weight_decay    = [0,1,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usar n_componentes dependiendo de cuanta varianza conservamos\n",
    "\n",
    "for pca_n in [3,5,10,15]:\n",
    "    train_data = ml.FootballMatchesDataset('train')\n",
    "\n",
    "    pca = PCA(n_components=pca_n,random_state=0)\n",
    "    train_data.data = torch.tensor(pca.fit_transform(train_data.data)).float()\n",
    "\n",
    "    model       = NeuralNetwork(pca_n,3)\n",
    "\n",
    "    # # SGD - 10 epochs\n",
    "    # _,_,_,_ = ml.Grid_Search_SGD(train_data,scalers,\n",
    "    #                                         criterions,learning_rate,momentum,model,\n",
    "    #                                         kfold,batch_size=bs,epochs=20)\n",
    "    # Adam - 10 epochs\n",
    "    _,_,_,_ = ml.Grid_Search_Adam(train_data,scalers,\n",
    "                                            criterions,learning_rate,b1,b2,model,\n",
    "                                            kfold,batch_size=bs,weight_decay=weight_decay,epochs=10)\n",
    "    # Adam - 20 epochs\n",
    "    _,_,_,_ = ml.Grid_Search_Adam(train_data,scalers,\n",
    "                                            criterions,learning_rate,b1,b2,model,\n",
    "                                            kfold,batch_size=bs,weight_decay=weight_decay,epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGUIENTES PASOS:\n",
    "\n",
    "# 1. otros optimizadores y epochs\n",
    "# 2. reducir dimensionalidad / seleccionar caract.\n",
    "# 3. otra arquitectura de red\n",
    "    # a. reducir params\n",
    "    # b. añadir hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp15_sgd_ep20_pca3_error = np.load(path_results+'mlp15_sgd_ep20_pca3//error__07_30_15_03_52.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp15_sgd_ep20_pca3_error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_lastepoch = mlp15_sgd_ep20_pca3_error[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(acc_test_lastepoch,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv          = acc_test_lastepoch.argmin(axis=1)\n",
    "best_config_cv   = np.unique(acc_test_lastepoch.argmin(axis=0))\n",
    "\n",
    "# best configurations are:\n",
    "print('config','\\t', '\\terror')\n",
    "\n",
    "for c in best_config_cv:\n",
    "    print(c,'\\t', acc_test_lastepoch[c,best_cv[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ml)\n",
    "\n",
    "ml.plotError(mlp15_sgd_ep20_pca3_error,best_config_cv,best_cv,'MLP 1x5 SGD PCA3',filename='mlp15_sgd_ep20_pca3_error',save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero entrenaremos varios modelos con diferentes características seleccionadas con la técnica de `Variance Threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = ml.FootballMatchesDataset(file = 'train')\n",
    "test_data   = ml.FootballMatchesDataset(file = 'test')\n",
    "\n",
    "old_data = train_data.data\n",
    "\n",
    "thresholds = [0.1,0.15,0.3,0.4]\n",
    "\n",
    "X_mean = torch.mean(train_data.data,dim=0).numpy()\n",
    "X_norm = train_data.data / X_mean\n",
    "\n",
    "len(X_norm), len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = feature_selection.VarianceThreshold(0.15)\n",
    "filter.fit(X_norm)\n",
    "mask_new_feat = filter.get_support()\n",
    "data.columns[mask_new_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 5\n",
    "kfold = model_selection.KFold(n_splits=folds,shuffle=True,random_state=0)\n",
    "\n",
    "temp = datetime.now().strftime(\"_%m_%d_%H_%M_%S\")\n",
    "\n",
    "for t in thresholds:\n",
    "    ml.log = {}\n",
    "\n",
    "    filter = feature_selection.VarianceThreshold(t)\n",
    "    filter.fit(X_norm)\n",
    "    _ = filter.transform(X_norm)\n",
    "    mask_new_feat = filter.get_support()\n",
    "    train_data.data = old_data[:,mask_new_feat]\n",
    "    print(f'\\nCon threshold: {np.sum(mask_new_feat)}')\n",
    "\n",
    "    # train\n",
    "    model = NeuralNetwork(np.sum(mask_new_feat),3)\n",
    "\n",
    "    train_data.data = (preprocessing.MinMaxScaler()\n",
    "                        .fit_transform(train_data.data).astype(np.float32))\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(),lr=0.1,betas=(0.01,0.99),weight_decay=0)\n",
    "\n",
    "    er, ac_tr, ac_te, cm = ml.train_wCrossValidation(model,nn.BCELoss(), opt, train_data, \n",
    "                                    kfold, epochs=100,bat_size=32)\n",
    "\n",
    "    ml.save_logging(temp,f'_thres_{t}')\n",
    "    ml.save_score(er,ac_tr,ac_te,cm,[],temp=temp,title=f'_{np.sum(mask_new_feat)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which features to select?\n",
    "print(data.columns[mask_new_feat])\n",
    "print(f'\\n{np.sum(mask_new_feat)} selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.clf()\n",
    "color = [('#6CF570' if m else '#FA3728') for m in mask_new_feat]\n",
    "plt.bar(data.columns, torch.var(X_norm,dim=0), width=0.75, color=color)\n",
    "plt.title(\"Normalized dividing by the mean w/ threshold=0.15\")\n",
    "plt.suptitle(\"Normalized feature variances\",fontsize=15)\n",
    "plt.xlabel(\"Feature name\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.savefig(path_graphs + 'divmean_feature_variances.jpg', format='jpg', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest_chi2 = feature_selection.SelectKBest(score_func=feature_selection.f_classif,k=10)\n",
    "kbest_chi2.fit(X_norm, train_data.labels)\n",
    "X_kbest = kbest_chi2.transform(X_norm)\n",
    "mask_new_feat = kbest_chi2.get_support()\n",
    "data.columns[mask_new_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELECTED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_selected_features = [False, False, True, True, True,\n",
    "       True,True, True, False, False,\n",
    "       False, False, True, True,\n",
    "       False, False, True, True, True,\n",
    "       True]\n",
    "\n",
    "data.columns[:-2][mask_selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II) Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different MLP Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilayer Perceptron with one hidden layer.\n",
    "\n",
    "We will train models with different number of units in its hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling/normalization\n",
    "scalers = [None,preprocessing.MinMaxScaler(), preprocessing.Normalizer()]\n",
    "\n",
    "# loss function\n",
    "# weights_class = torch.tensor(np.mean(train_data.labels.numpy(),axis=0))\n",
    "criterions = [nn.BCELoss()]\n",
    "\n",
    "# optimizer\n",
    "learning_rate = [1,1e-1,1e-2]\n",
    "\n",
    "# cross-validation\n",
    "folds = 5\n",
    "kfold = model_selection.KFold(n_splits=folds,shuffle=True,random_state=0)\n",
    "\n",
    "# batch-size\n",
    "bs = [32,64,128]\n",
    "\n",
    "##################\n",
    "# SGD hyperparams\n",
    "\n",
    "momentum = [True,False]\n",
    "\n",
    "# Adam hyperparams\n",
    "r = np.random.rand\n",
    "b1, b2          = [0.01,0.9], [0.1,0.9,0.99]\n",
    "weight_decay    = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ml)\n",
    "temp = datetime.now().strftime(\"_%m_%d_%H_%M_%S\")\n",
    "units_array = [3,10]\n",
    "\n",
    "train_data  = ml.FootballMatchesDataset(file = 'train')\n",
    "old_data = train_data.data\n",
    "X_mean = torch.mean(train_data.data,dim=0).numpy()\n",
    "X_norm = train_data.data / X_mean\n",
    "\n",
    "for units in units_array:\n",
    "\n",
    "    # train\n",
    "    model = NeuralNetwork(train_data.data.shape[1],3,hidden_neurons=units)\n",
    "\n",
    "    _,_,_,_ = ml.Grid_Search_SGD(train_data,scalers,criterions,learning_rate,momentum,model,\n",
    "                                  kfold,batch_size=bs,epochs=100,\n",
    "                                  root=path_results+f'sgd_mlp_{units}_{temp}//')\n",
    "\n",
    "    _,_,_,_ = ml.Grid_Search_Adam(train_data,scalers,criterions ,learning_rate,b1,b2,model,\n",
    "                                  kfold,batch_size=bs,weight_decay=weight_decay,epochs=100,\n",
    "                                  root=path_results+f'adam_mlp{units}_{temp}//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5 , 0.5 ],\n",
       "       [0.99, 0.5 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "aux = [(.5,.5),(.99,.5)]\n",
    "np.array(aux)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('tfg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "288ff6fe8157f43ba3e1fb4cfa0011490e9beb907b54ad0d71ae70a61946bdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
