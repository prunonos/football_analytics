{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import feature_selection, preprocessing, discriminant_analysis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rawdata    = 'F://TFG//datasets/raw_datasets//'\n",
    "path_train      = 'F://TFG//datasets//data_train//'\n",
    "path_graphs     = 'F://TFG//graphs//'\n",
    "path            = 'F:/TFG/datasets/nature-dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ataque_defensa   = pd.read_csv(path_rawdata+'indicador_ataquedefensa_teams.csv',sep=';')\n",
    "data             = pd.read_csv(path_rawdata+'matches_wUltPartidos.csv',sep=';',index_col='wyId')\n",
    "shots            = pd.read_json(path_rawdata + 'shots.json')\n",
    "passes           = pd.read_json(path_rawdata + 'passes.json')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el dataset base sobre el cual haremos un analisis de características. Las características se dividen en diferentes tipos:\n",
    "- **Características propias del partido (lo identifican)**<br>\n",
    "       ['matchId', 'dateutc', 'competitionId', 'seasonId', 'roundId', 'winner',\n",
    "       'teamId_home', 'score_home', 'scoreHT_home', 'teamId_away',\n",
    "       'score_away', 'scoreHT_away']\n",
    "\n",
    "\n",
    "- **Estadísticas del partido**: p.e. goles, faltas, corners... Las usaremos para visualizar y ver si hay dependencia con el resultado<br>\n",
    "       ['HS', 'AS', 'HST', 'AST', 'HC', 'AC',\n",
    "       'HF', 'AF']\n",
    "\n",
    "\n",
    "- **Cuotas de Cassas de Apuestas**: usadas en el Baseline<br> ['B365H', 'B365D', 'B365A']\n",
    "\n",
    "\n",
    "- **Más estadísticas**: Las usaremos para visualizar y ver si hay dependencia con el resultado <br>\n",
    "       ['y0_sh_H', 'x0_sh_H',\n",
    "       'acc_avg_sh_H', 'goal_avg_sh_H', 'y0_sh_A', 'x0_sh_A', 'acc_avg_sh_A',\n",
    "       'goal_avg_sh_A', 'y0_ps_H', 'x0_ps_H', 'y1_ps_H',\n",
    "       'x1_ps_H', 'acc_avg_ps_H', 'keypass_ps_H', 'y0_ps_A', 'x0_ps_A',\n",
    "       'y1_ps_A', 'x1_ps_A', 'acc_avg_ps_A', 'keypass_ps_A']\n",
    "\n",
    "       \n",
    "- **Estadísticas del 11 titular los ultimos N partidos**: usadas para el modelo de predicción<br>\n",
    "       ['mins4_H', 'mins4_A', 'shots_11H',\n",
    "       'shots_11A', 'shots_acc_11H', 'shots_acc_11A', 'goals_H', 'goals_A',\n",
    "       'passes_11H', 'passes_11A', 'passes_acc_11H', 'passes_acc_11A',\n",
    "       'keyPasses_H', 'keyPasses_A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESAMIENTO DEL DATASET\n",
    "\n",
    "Ligeros cambios para visualizar y analizar los datos de los que disponemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "\n",
    "for match in data.itertuples():\n",
    "    if match.winner == 0: target.append(0)\n",
    "    elif match.winner == match.teamId_home: target.append(1)\n",
    "    else: target.append(2)\n",
    "\n",
    "data['res'] = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALISIS Y VISUALIZACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadísticas del partido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribución de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importante también visualizar separando las diferentes competiciones, para ver si siguen las mismas distribuciones o no.\n",
    "\n",
    "También buena idea visualizar dependiendo el nivel de los clubes. Para ello podemos hacer clusters de los equipos según las características (goles marcados y encajados por ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('La distribución de victorias locales, empates y victores visitantes es la siguiente:')\n",
    "clases = ['Empate','Victorias locales','Victorias visitantes']\n",
    "for clase,count in zip(clases,np.bincount(data.res)):\n",
    "    print(f'{clase}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribución de los goles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(18,6))\n",
    "\n",
    "fig.suptitle('Distribución de los goles', fontsize=20)\n",
    "\n",
    "axs[0].set_title('Distribucion de goles locales')\n",
    "axs[1].set_title('Distribucion de goles visitantes')\n",
    "\n",
    "max_value = max(np.concatenate([data.score_home, data.score_away]))\n",
    "\n",
    "idxs = ['score_home','score_away']\n",
    "for i,ax in zip(idxs,axs):\n",
    "    x = data[i]\n",
    "    ax.hist(x,bins=np.arange(max_value+1)-0.5,rwidth=0.8,align='mid')\n",
    "    ax.set_ylim(0,550)\n",
    "    ax.set_xlim([-1,max_value+1])\n",
    "    ax.set_xticks(range(max_value+1))\n",
    "    ax.set_xlabel('# goles')\n",
    "    ax.set_ylabel('# partidos')\n",
    "\n",
    "# plt.savefig(path_graphs + 'goals_distribution.pdf', format='pdf')\n",
    "plt.savefig(path_graphs + 'goals_distribution.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1,figsize=(12,9))\n",
    "\n",
    "fig.suptitle('Distribución de los disparos', fontsize=20)\n",
    "\n",
    "axs[0].set_title('Distribucion de disparos locales')\n",
    "axs[1].set_title('Distribucion de disparos visitantes')\n",
    "\n",
    "max_value = max(np.concatenate([data.HS, data.AS]))\n",
    "\n",
    "idxs = ['HS','AS']\n",
    "for i,ax in zip(idxs,axs):\n",
    "    x = data[i]\n",
    "    ax.hist(x,bins=np.arange(max_value+1)-0.5,rwidth=0.8,align='mid')\n",
    "    ax.set_ylim(0,150)\n",
    "    ax.set_xlim([-1,max_value+1])\n",
    "    ax.set_xticks(range(max_value+1))\n",
    "    ax.set_xlabel('# disparos')\n",
    "    ax.set_ylabel('# partidos')\n",
    "    # ax.grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(path_graphs + 'goals_distribution.pdf', format='pdf')\n",
    "plt.savefig(path_graphs + 'shots_distribution.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_HS = np.mean(data.HS)\n",
    "std_HS  = np.std(data.HS)\n",
    "mean_AS = np.mean(data.AS)\n",
    "std_AS  = np.std(data.AS)\n",
    "\n",
    "print(f\"Los disparos locales siguen una distribución Normal con \\u03BC={mean_HS} y \\u03C3={std_HS}.\")\n",
    "print(f\"Los disparos visitantes siguen una distribución Normal con \\u03BC={mean_AS} y \\u03C3={mean_AS}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QQ-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizar:\n",
    "- distribución de disparos del equipo que gana\n",
    "- distribucion de disparos del equipo que pierde\n",
    "- distribucion de la zona del campo de los disparos (total, local y visitante)\n",
    "- distribucion de la zona del campo de los pases (total, local y visitante)\n",
    "- Zona del campo en la que hace los pases el equipo ganador y perdedor (heatmap) [necesario el dataset de pases]\n",
    "- Zona del campo en la que dispara el equipo ganador y perdedor (heatmap) [necesario el dataset de disparos]\n",
    "- Probabilidad de que el equipo más acertado cara a puerta gana el partido\n",
    "- Probabilidad de que el equipo más acertado en pases gana el partido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion de disparos del equipo que gana y del que pierde\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribucion de disparos del equipo que gana y del que pierde\n",
    "\n",
    "num_shots = shots.set_index(['teamId','matchId']).groupby(['teamId','matchId']).agg(shots=('id','count')).reset_index()\n",
    "num_shots.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_ids, match_ids = (list(data[data.winner!=0].winner), list(data[data.winner!=0].matchId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners_match = pd.DataFrame({'teamId':winner_ids, 'matchId':match_ids})\n",
    "winners_match.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_shots = pd.merge(winners_match,num_shots, on=['teamId','matchId'], how='inner')\n",
    "winner_shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.unique(winner_shots.matchId).shape[0] == len(match_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pierde_h, matches_pierde_h = data[data.res == 2].teamId_home, data[data.res == 2].matchId         # cogemos el contrario -> si gana visitante (2) cogemos el local\n",
    "pierde_a, matches_pierde_a = data[data.res == 1].teamId_away, data[data.res == 1].matchId\n",
    "\n",
    "pierde = np.concatenate([pierde_h,pierde_a])\n",
    "matches_pierde = np.concatenate([matches_pierde_h, matches_pierde_a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losers_match = pd.DataFrame({'teamId':pierde, 'matchId':matches_pierde})\n",
    "losers_match.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loser_shots = pd.merge(losers_match,num_shots, on=['teamId','matchId'], how='inner')\n",
    "loser_shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1,figsize=(12,9))\n",
    "\n",
    "fig.suptitle('Distribución de los disparos', fontsize=20)\n",
    "\n",
    "axs[0].set_title('Distribucion de disparos del equipo ganador')\n",
    "axs[1].set_title('Distribucion de disparos del equipo perdedor')\n",
    "\n",
    "max_value = max(np.concatenate([winner_shots.shots, loser_shots.shots]))\n",
    "\n",
    "# idxs = ['HS','AS']\n",
    "x = [winner_shots.shots,loser_shots.shots]\n",
    "\n",
    "for i,ax in enumerate(axs):\n",
    "    ax.hist(x[i],bins=np.arange(max_value+1)-0.5,rwidth=0.8,align='mid')\n",
    "    ax.set_ylim(0,130)\n",
    "    ax.set_xlim([-1,max_value+1])\n",
    "    ax.set_xticks(range(max_value+1))\n",
    "    ax.set_xlabel('# disparos')\n",
    "    ax.set_ylabel('# partidos')\n",
    "    # ax.grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# plt.savefig(path_graphs + 'goals_distribution.pdf', format='pdf')\n",
    "plt.savefig(path_graphs + 'shots_distribution_winner_losers.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relacion de los disparos del equipo ganador y perdedor con el resultado del partido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.merge(winner_shots,loser_shots,on='matchId', suffixes=['_win','_los'])\n",
    "x = pd.merge(x,res,on='matchId')\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.grid()\n",
    "plt.title('Distribucion de disparos en relación con \\nlos disparos del equipo ganador y perdedor', fontsize=15)\n",
    "plt.xlabel('# disparos eq. ganador')\n",
    "plt.ylabel('# disparos eq. perdedor')\n",
    "plt.scatter(x=x.shots_win, y=x.shots_los,c=(x.shots_win > x.shots_los),cmap='RdYlBu',alpha=0.3)\n",
    "# plt.xlim, plt.ylim = (0,40), (0,40)\n",
    "\n",
    "plt.savefig(path_graphs + 'shots_winloss_dist.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cual es la probabilidad de que un equipo que ha realizado más disparos pierda un partido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(x.shots_win < x.shots_los)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **32%** de los partidos los gana el equipo que menos disparos ha realizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = max(x.shots_win-x.shots_los)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.xticks(range(max_value+1))\n",
    "plt.hist(x.shots_win-x.shots_los, bins=np.arange(max_value+1)-0.5, rwidth=0.8, align='mid')\n",
    "plt.ylim = (0,80)\n",
    "plt.xlim = ([-1,max_value+1])\n",
    "\n",
    "plt.title('Distribucion de la diferencia de disparos\\n entre el ganador y el perdedor.')\n",
    "plt.xlabel('# diferencia de disparos')\n",
    "plt.ylabel('# partidos')\n",
    "# plt.grid()\n",
    "\n",
    "plt.savefig(path_graphs + 'dif_shots_winlos.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value = max(x[x.res==1].shots_win-x[x.res==1].shots_los)\n",
    "mask = x.shots_win < x.shots_los\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.xticks(range(max_value+1))\n",
    "plt.hist(x[mask].shots_los-x[mask].shots_win, bins=np.arange(max_value+1)-0.5, rwidth=0.8, align='mid')\n",
    "plt.ylim = (0,80)\n",
    "plt.xlim = ([-1,max_value+1])\n",
    "\n",
    "plt.title('Distribucion de la diferencia de disparos\\n cuando el ganador ha realizado menos disparos.')\n",
    "plt.xlabel('# diferencia de disparos')\n",
    "plt.ylabel('# partidos')\n",
    "# plt.grid()\n",
    "\n",
    "plt.savefig(path_graphs + 'dif_shots_los_mas_disparos.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acierto en el disparo y en el pase en relación con el numero de partidos ganados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.read_csv(path_rawdata+'teams.csv',index_col='wyId',delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_shots_teams = shots.groupby('teamId').agg(accuracy=('accuracy','mean')).sort_values('accuracy',ascending=False)\n",
    "acc_passes_teams = passes.groupby('teamId').agg(accuracy=('accuracy','mean')).sort_values('accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_shots_teams_names = acc_shots_teams.join(teams)[['name','accuracy']]\n",
    "acc_shots_teams_names['index'] = range(1,len(acc_shots_teams_names)+1)\n",
    "acc_shots_teams_names.set_index('index',inplace=True)\n",
    "\n",
    "acc_passes_teams_names = acc_passes_teams.join(teams)[['name','accuracy']]\n",
    "acc_passes_teams_names['index'] = range(1,len(acc_passes_teams_names)+1)\n",
    "acc_passes_teams_names.set_index('index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_shots_teams_names[:10], acc_shots_teams_names[-10:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_passes_teams_names[:10], acc_passes_teams_names[-10:][::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relacion entre la media de goles encajados y marcados por un equipo\n",
    "\n",
    "Vamos a plotear el indicador ataque-defensa de ambos equipos respecto al resultado final del partido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ataque_defensa.set_index('teamId').join(teams[['name']]).sort_values(by='indicador',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ataque_defensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ataque_defensa.indicador\n",
    "\n",
    "max_value = int(max(x))+1\n",
    "min_value = int(min(x))\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.xticks(range(min_value,max_value+1))\n",
    "plt.hist(x, bins=np.arange(min_value,max_value+1)-0.5, rwidth=0.8, align='mid')\n",
    "plt.ylim = (0,20)\n",
    "plt.xlim = ([min_value,max_value+1])\n",
    "\n",
    "plt.title('Distribucion del indicador Ataque-Defensa.')\n",
    "plt.xlabel('valor indicador')\n",
    "plt.ylabel('# equipos')\n",
    "# plt.grid()\n",
    "\n",
    "plt.savefig(path_graphs + 'dist_ataquedefensa.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.ataque_defensa_h.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.ataque_defensa_h\n",
    "\n",
    "max_value = max(x)\n",
    "min_value = min(x)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.xticks(np.arange(int(min_value),int(max_value)+2,step=0.5))\n",
    "plt.hist(x, bins=np.arange(int(min_value),int(max_value)+2,step=0.25)-0.25, rwidth=0.8, align='mid')\n",
    "plt.ylim = (0,250)\n",
    "plt.xlim = ([int(min_value),int(max_value)+2])\n",
    "\n",
    "plt.title('Distribucion del indicador Ataque-Defensa.')\n",
    "plt.xlabel('valor indicador')\n",
    "plt.ylabel('# equipos')\n",
    "# plt.grid()\n",
    "\n",
    "plt.savefig(path_graphs + 'dist_ataquedefensa.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['res','ataque_defensa_season_h','ataque_defensa_season_a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "# plt.grid()\n",
    "plt.title('Relacion entre el resultado y el indicador Ataque-Defensa de la temporada', fontsize=15)\n",
    "plt.xlabel('Ataque-Defensa local')\n",
    "plt.ylabel('Ataque-Defensa visitante')\n",
    "plt.scatter(x=x.ataque_defensa_season_h, y=x.ataque_defensa_season_a,c=x.res,cmap='Dark2_r',alpha=0.4)\n",
    "# plt.xlim, plt.ylim = (0,40), (0,40)\n",
    "\n",
    "plt.savefig(path_graphs + 'dist_res_ataquedefensa.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['res','ataque_defensa_h','ataque_defensa_a']]\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "# plt.grid()\n",
    "plt.title('Relacion entre el resultado y el indicador Ataque-Defensa de los ultimos 4 partidos', fontsize=15)\n",
    "plt.xlabel('Ataque-Defensa local')\n",
    "plt.ylabel('Ataque-Defensa visitante')\n",
    "plt.scatter(x=x.ataque_defensa_h, y=x.ataque_defensa_a,c=x.res,cmap='Dark2_r',alpha=0.4)\n",
    "# plt.xlim, plt.ylim = (0,40), (0,40)\n",
    "\n",
    "plt.savefig(path_graphs + 'dist_res_ataquedefensa.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿ CAMBIAR NORMALIZACIÓN ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadisticas del 11 titular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enlaces interesantes:**\n",
    "\n",
    "How to Choose a Feature Selection Method For Machine Learning\n",
    "https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n",
    "\n",
    "Feature Selection Sklearn\n",
    "https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
    "\n",
    "Chi-Square Test for Feature Selection in Machine learning\n",
    "https://towardsdatascience.com/chi-square-test-for-feature-selection-in-machine-learning-206b1f0b8223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion de los minutos jugados por el 11 titular en los ultimos 4 partidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although standardization and normalization have the same basic function, they utilize different approaches. As a result, their use cases differ as well.\n",
    "\n",
    "Standardization is ideal for data that fits a normal/gaussian distribution.\n",
    "\n",
    "It is also superior when handling data with outliers as it is more resistant to extreme values. Standardization is often used in PCA, where the aim is to maximize variance while reducing dimensionality.\n",
    "\n",
    "Normalization, on the other hand, is the safer alternative when you are unsure of the distribution of your data.\n",
    "\n",
    "All in all, determining the best feature scaling method in a machine learning task requires a strong understanding of the data being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(['y0_sh_H', 'x0_sh_H',\n",
    "       'acc_avg_sh_H', 'goal_avg_sh_H', 'y0_sh_A', 'x0_sh_A', 'acc_avg_sh_A',\n",
    "       'goal_avg_sh_A', 'mins4_H', 'mins4_A', 'y0_ps_H', 'x0_ps_H', 'y1_ps_H',\n",
    "       'x1_ps_H', 'acc_avg_ps_H', 'keypass_ps_H', 'y0_ps_A', 'x0_ps_A',\n",
    "       'y1_ps_A', 'x1_ps_A', 'acc_avg_ps_A', 'keypass_ps_A', 'shots_11H',\n",
    "       'shots_11A', 'shots_acc_11H', 'shots_acc_11A', 'goals_H', 'goals_A',\n",
    "       'passes_11H', 'passes_11A', 'passes_acc_11H', 'passes_acc_11A',\n",
    "       'keyPasses_H', 'keyPasses_A', 'ataque_defensa_h', 'ataque_defensa_a',\n",
    "       'ataque_defensa_season_h', 'ataque_defensa_season_a'])\n",
    "\n",
    "features = features[:-4] # de momento descartamos ataque_defensa\n",
    "\n",
    "X = data[features] # descartamos de momento las de ataque_defensa\n",
    "\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminamos las filas con valores nulos\n",
    "\n",
    "X = X.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necesario NORMALIZAR ???\n",
    "# normalization / minmax scale / other ???\n",
    "# normalizamos dividiendo cada feature por su media\n",
    "\n",
    "# X_norm = preprocessing.minmax_scale(X,axis=0)\n",
    "X_mean = X.mean(axis=0).to_numpy()\n",
    "X_norm = X / X_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi Univariado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Descartamos el uso de ${\\chi}^2$ por ser un test usado para inputs (variables) y outputs categóricas.\n",
    "\n",
    "Puede ser interesante el uso de métodos de Sklearn como *SelectKBest* o *SelectPercentile*, que dados una función que puntua cada variable, selecciona las mejores variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance is the measurement of the spread between numbers in a variable. It measures how far a number is from the mean and every number in a variable.\n",
    "\n",
    "The variance of a feature determines how much it is impacting the response variable. If the variance is low, it implies there is no impact of this feature on response and vice-versa.\n",
    "\n",
    "VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# que valor asignar al Threshold ? Hay alguna manera de testear ? \n",
    "# Despues de normalizar las características pierden muucha varianza\n",
    "\n",
    "filter = feature_selection.VarianceThreshold(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter.fit(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = filter.transform(X_norm)\n",
    "mask_new_feat = filter.get_support()\n",
    "pd.DataFrame(new_X,columns=features[mask_new_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.clf()\n",
    "plt.bar(features, X_norm.var(), width=0.75)\n",
    "plt.title(\"Normalized feature variances\")\n",
    "plt.xlabel(\"Feature name\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(path_graphs + 'normalized_feature_variances.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[mask_new_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas son las características que tienen más de 0.5 de Variancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Ratio Dispersión (RMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Mean_absolute_difference#Relative_mean_absolute_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.goals_H.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gmean(X_norm).reshape(1,-1), columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ANOVA f-test Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://towardsdatascience.com/anova-for-feature-selection-in-machine-learning-d9305e228476\n",
    "\n",
    "**F-Distribution?**\n",
    "A probability distribution generally used for the analysis of variance. It assumes Hypothesis as\n",
    "\n",
    "- H0: two variances are equal\n",
    "- H1: two variances are not equal\n",
    "\n",
    "**Degrees of Freedom**\n",
    "refers to the maximum number of logically independent values, which have the freedom to vary. In simple words, it can be defined as the total number of observations minus the number of independent constraints imposed on the observations.\n",
    "\n",
    "$D_f = N-1$ where $N$ is the sample size.  \n",
    "\n",
    "**F-value**\n",
    "\n",
    "$s$ = variance\n",
    "\n",
    "${\\chi}^2$  = $\\frac{(n-1) s^2}{\\sigma^2}$\n",
    "\n",
    "$F = \\frac{\\chi_1^2 / (n_1 - 1)}{\\chi_2^2 / (n_2 - 1)}$\n",
    "\n",
    "**ANOVA**\n",
    "\n",
    "**An**alysis **o**f **Va**riance is a statistical method, used to check the means of two or more groups that are significantly different from each other. It assumes Hypothesis as\n",
    "- H0: means of all groups are equal\n",
    "- H1: at least one mean of the groups are different\n",
    "\n",
    "ANOVA is a parametric statistical hypothesis test for determining whether the means from two or more samples of data (often three or more) come from the same distribution or not. \n",
    "\n",
    "An F-statistic, or F-test, is a class of statistical tests that calculate the ratio between variances values\n",
    "\n",
    "**The results of this test can be used for feature selection where those features that are independent of the target variable can be removed from the dataset.**\n",
    "\n",
    "**Calculation:**\n",
    "\n",
    "Total variance of the data.\n",
    "\n",
    "$SST = \\sum_{i}{ ( x^{(i)} - \\overline{x} )^2 }$\n",
    "\n",
    "Variance between different groups $g$.\n",
    "\n",
    "$SSB = \\sum_{g}{ ( \\overline{x_g} - \\overline{x} )^2 }$\n",
    "\n",
    "Variance within each group.\n",
    "\n",
    "$SSW = \\sum_{g}[\\sum_{i\\in{g}}{ ( x^{(i)} - \\overline{x_g} )^2 }]$\n",
    "\n",
    "Distance between each observed value $x_g^{(i)}$ within the group $g$, with the group-mean $\\overline{x_g}$\n",
    "\n",
    "$SST = SSB + SSE$\n",
    "\n",
    "So, the F value will be the comparison of the variance between the groups and the variance within the groups.\n",
    "\n",
    "$F = \\frac{SSB / df_b}{SSW / df_w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculo manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSB = np.sum( (np.mean(X,axis=0) - np.mean(X.to_numpy().flatten()))**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSW = np.sum(np.sum( (X - np.mean(X,axis=0))**2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb, dfw = X.shape[1]-1, X.shape[0]-1\n",
    "dfb,dfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = (SSB/dfb)/(SSW/dfw) \n",
    "F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con  95% confianza, $\\alpha$ = 0.05, $df_b$ = 33 y $df_w$ = 1430, y dados que nuestro F-value de nuestra F-table es **1.4445** y nuestro F-value es **0.3539**, entonces nuestra hipótesis nula es cierta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SelectKBest  $F-classif$\n",
    "\n",
    "Habrá que entrenar con Cross Validation los hiperparámetros, en este caso para encontrar el mejor valor de $k$.\n",
    "\n",
    "Vamos a usar *f_classif* que calcula la ANOVA F-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest_fclass = feature_selection.SelectKBest(score_func=feature_selection.f_classif,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest_fclass.fit(X_norm, data.dropna().res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kbest = kbest_fclass.transform(X_norm)\n",
    "mask_new_feat = kbest_fclass.get_support()\n",
    "pd.DataFrame(X_kbest,columns=features[mask_new_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[mask_new_feat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gran diferencia respecto a Variance Threshold es que SelectKBest selecciona como característica a tener en cuenta al *accuracy* del 11 titular en los pases. Sin embargo no tiene en cuenta los pases clave locales, solo los visitantes.\n",
    "\n",
    "Pero repito, habria que entrenar los hiperparámetros (*k*) para obtener el numero óptimo de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SelectKBest - $\\chi^2$\n",
    "\n",
    "$\\chi^2 = \\sum_{i}{ \\mathcal{N_i}^2}$\n",
    "\n",
    "Una variable aleatoria $\\chi$ sigue una distribucion chi-square si puede ser escrita como la suma de unas variables Normales al cuadrado.\n",
    "\n",
    "Un test chi-square es usado en estadística para probar la independencia de dos eventos.\n",
    "\n",
    "$\\chi^2_c = \\sum_{i}{\\frac{(O_i - E_i)^2}{E_i}}$\n",
    "\n",
    "$c$ = grados de libertad <br>\n",
    "$O$ = valor(es) observados <br>\n",
    "$E$ = valor(es) esperados\n",
    "\n",
    "En selección de características nuestro objetivo es seleccionar las características (features) que son más dependientes en el valor de target.\n",
    "\n",
    "Cuando dos características son independientes, el valor observado está muy próximo al esperado, por lo tanto tendremos un pequeño valor Chi-Square ($\\chi^2\\approx0$). Un alto valor de Chi-Square indica que la hipótesis de que las variables son independientes es incorrecta.\n",
    "\n",
    "Las limitaciones de Chi-Square es que es muy sensible a valor pequeños."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest_chi2 = feature_selection.SelectKBest(score_func=feature_selection.chi2,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest_chi2.fit(X_norm, data.dropna().res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kbest = kbest_chi2.transform(X_norm)\n",
    "mask_new_feat = kbest_chi2.get_support()\n",
    "pd.DataFrame(X_kbest,columns=features[mask_new_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[mask_new_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = -np.log10(kbest_chi2.pvalues_)\n",
    "pvalues /= pvalues.max()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.clf()\n",
    "plt.bar(features, pvalues, width=0.75)\n",
    "plt.title(\"Feature univariate score\")\n",
    "plt.xlabel(\"Feature number\")\n",
    "plt.ylabel(\"P-values\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(path_graphs + 'feature_univariate_score_chi2.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INTERESTING: compare with weights of our model (e.g. SVM)\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html#sphx-glr-auto-examples-feature-selection-plot-feature-selection-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual information between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n",
    "\n",
    "The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n",
    "\n",
    "Comparison of F-test and mutual information¶\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html#sphx-glr-auto-examples-feature-selection-plot-f-test-vs-mi-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisi Multivariado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Discriminant Analysis (LDA)\n",
    "\n",
    "<b>En LDA se asume que las caract. o muestras son independientes?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b\n",
    "\n",
    "LinearDiscriminantAnalysis can be used to perform supervised dimensionality reduction, by projecting the input data to a linear subspace consisting of the directions which maximize the separation between classes.\n",
    "\n",
    "with $x\\in\\R^d$\n",
    "\n",
    "$P(y=k | x) = \\frac{P(x | y=k) P(y=k)}{P(x)} = \\frac{P(x | y=k) P(y = k)}{ \\sum_{l} P(x | y=l) \\cdot P(y=l)}$\n",
    "\n",
    "for linear and quadratic discriminant analysis,  is modeled as a multivariate Gaussian distribution with density:\n",
    "\n",
    "$P(x | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k)\\right)$\n",
    "\n",
    "In LDA is a special case in which we assume to share the same covariance matrix: $\\Sigma_k = \\Sigma$ for all $k$.\n",
    "\n",
    "$\\log P(y=k | x) = -\\frac{1}{2} (x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k) + \\log P(y = k) + Cst.$\n",
    "\n",
    "The term $(x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k)$ corresponds to the Mahalanobis Distance between the sample $x$ and the mean $\\mu_k$. The Mahalanobis distance tells how close $x$ is from $\\mu_k$, while also accounting for the variance of each feature. We can thus interpret LDA as assigning $x$ to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities.\n",
    "\n",
    "The log-posterior of LDA can be also written as:\n",
    "\n",
    "$\\log P(y=k | x) = \\omega_k^t x + \\omega_{k0} + Cst.$\n",
    "\n",
    "where $\\omega_k = \\Sigma^{-1} \\mu_k$ and $\\omega_{k0} = -\\frac{1}{2} \\mu_k^t\\Sigma^{-1}\\mu_k + \\log P (y = k)$\n",
    "\n",
    "It is clear, from the above formula, that LDA has linear decision surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = discriminant_analysis.LinearDiscriminantAnalysis(solver='eigen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No podemos proyectar los datos a más de $k-1$ dimensiones, ya que la matrix $S_b$ (between-class matrix) tiene rango $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.dropna().res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(X_norm,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lda = lda.transform(X_norm)\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot LDA transformation\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('LDA projection')\n",
    "plt.xlabel('x projection')\n",
    "plt.ylabel('y projection')\n",
    "plt.legend(y)\n",
    "plt.scatter(X_lda[:,0],X_lda[:,1],c=y,cmap='Dark2_r',alpha=0.4)\n",
    "\n",
    "plt.savefig(path_graphs + 'lda_projection.jpg', format='jpg', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection in Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About Time Series: https://towardsdatascience.com/12-things-you-should-know-about-time-series-975a185f4eb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I. Pearson's Correlation Coef VS Kolmogorov-Smirnov Test\n",
    "\n",
    "https://towardsdatascience.com/time-series-clustering-and-dimensionality-reduction-5b3b4e84f6a3\n",
    "\n",
    "Kolmogorov-Smirnov Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### II. Vector Autoregression (VAR)\n",
    "\n",
    "https://towardsdatascience.com/multiple-series-forecast-them-together-with-any-sklearn-model-96319d46269\n",
    "\n",
    "¿How many lags to use? https://www.econometrics-with-r.org/14-6-llsuic.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### III. ForeCA\n",
    "\n",
    "https://stats.stackexchange.com/questions/82291/time-series-dimensionality-reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA)\n",
    "\n",
    "Sklearn does not implement PCA with eigen-decomposition of the Covariance matrix. However it is performed via SVD of the data matrix X.\n",
    "\n",
    "https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of Principal Components (n_comps) will be also trained as hyperparameter during Cross-Validation."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "288ff6fe8157f43ba3e1fb4cfa0011490e9beb907b54ad0d71ae70a61946bdb3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('tfg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
