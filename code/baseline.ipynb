{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import mytrain_lib as ml\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix as confmat\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV,KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_rawdata    = 'F://TFG//datasets/raw_datasets//'\n",
    "path_train      = 'F://TFG//datasets//data_train//'\n",
    "path_graphs     = 'F://TFG//graphs//'\n",
    "path            = 'F:/TFG/datasets/nature-dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data             = pd.read_csv(path_rawdata+'matches_wUltPartidos.csv',sep=';',index_col='wyId')\n",
    "\n",
    "target = []\n",
    "\n",
    "for match in data.itertuples():\n",
    "    if match.winner == 0: target.append(0)\n",
    "    elif match.winner == match.teamId_home: target.append(1)\n",
    "    else: target.append(2)\n",
    "\n",
    "data['res'] = target\n",
    "data['matchId'] = data.index\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = ml.FootballMatchesDataset(file = 'train')\n",
    "test_data   = ml.FootballMatchesDataset(file = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainmatches = train_data.matches\n",
    "testmatches  = test_data.matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_odds_res(X):\n",
    "    odds = [1/X.B365D.to_numpy(),1/(X.B365H.to_numpy()),1/(X.B365A.to_numpy())]\n",
    "    odds = np.array(odds).T\n",
    "    res = np.array(X.res)\n",
    "    return odds,res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline I: max odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds,res = get_odds_res(data.loc[trainmatches])\n",
    "pred = np.argmax(odds,axis=1)\n",
    "assert len(pred) == len(res)\n",
    "np.mean(pred==res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.dispConfusionMatrix(confmat(res,pred,labels=[0,1,2]),'Baseline max odds training confusion matrix',filename='confmat_base_maxodds_train',save=True,size=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds,res = get_odds_res(data.loc[testmatches])\n",
    "pred = np.argmax(odds,axis=1)\n",
    "assert len(pred) == len(res)\n",
    "np.mean(pred==res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.dispConfusionMatrix(confmat(res,pred,labels=[0,1,2]),'Baseline max odds test confusion matrix',filename='confmat_base_maxodds_test',save=True,size=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline II: SVM Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds,res = get_odds_res(data.loc[trainmatches])\n",
    "rand_list = {\"C\": uniform(2, 10),\n",
    "             \"gamma\": uniform(0.1, 1),\n",
    "             \"degree\": uniform(2,10),\n",
    "             \"kernel\":['linear','poly','rbf']}\n",
    "\n",
    "svm = SVC(random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search = RandomizedSearchCV(svm, param_distributions=rand_list, n_iter=50, n_jobs=4, cv=3, random_state=0) \n",
    "rand_search.fit(odds,res)\n",
    "cv_results = pd.DataFrame(rand_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.sort_values('rank_test_score')[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(cv_results.mean_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds,res = get_odds_res(data.loc[testmatches])\n",
    "pred = rand_search.best_estimator_.predict(odds)\n",
    "assert len(pred) == len(res)\n",
    "np.mean(pred==res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.dispConfusionMatrix(confmat(res,pred,labels=[0,1,2]),'Baseline svm test confusion matrix',filename='confmat_base_svm_test',save=True,size=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline III: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds,res = get_odds_res(data.loc[trainmatches])\n",
    "rand_list = {\"n_estimators\": randint(5, 200),\n",
    "             \"max_depth\": randint(2, 10),\n",
    "             \"min_samples_split\": randint(2,100),\n",
    "             \"min_samples_leaf\": randint(1,100),\n",
    "             \"max_leaf_nodes\": randint(3,20)}\n",
    "\n",
    "randforest = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search = RandomizedSearchCV(randforest, param_distributions=rand_list, n_iter=50, n_jobs=4, cv=3, random_state=0) \n",
    "rand_search.fit(odds,res)\n",
    "cv_results = pd.DataFrame(rand_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.sort_values('rank_test_score')[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(cv_results.mean_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds,res = get_odds_res(data.loc[testmatches])\n",
    "pred = rand_search.best_estimator_.predict(odds)\n",
    "assert len(pred) == len(res)\n",
    "np.mean(pred==res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.dispConfusionMatrix(confmat(res,pred,labels=[0,1,2]),'Baseline random forest test confusion matrix',filename='confmat_base_randforest_test',save=True,size=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline IV: Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BettingDataset(Dataset):\n",
    "    def __init__(self,odds,res):\n",
    "        self.data       = torch.tensor(odds).float()\n",
    "        self.labels     = F.one_hot(torch.tensor(res),num_classes=3).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        sample  = self.data[idx]\n",
    "        label   = self.labels[idx]\n",
    "        return sample, label, -1\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_feature, ouput_classes, hidden_neurons=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.h1 = nn.Linear(in_features=input_feature,out_features=hidden_neurons)\n",
    "        self.bn = nn.BatchNorm1d(hidden_neurons)\n",
    "        self.out = nn.Linear(hidden_neurons,ouput_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.h1(x)\n",
    "        x = F.relu(self.bn(x))\n",
    "        return F.softmax(self.out(x),1)    \n",
    "\n",
    "    def reset_weights(self):\n",
    "        self.h1.reset_parameters()\n",
    "        self.bn.reset_parameters()\n",
    "        self.out.reset_parameters()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(3,3,3)\n",
    "train_data = BettingDataset(odds,res)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "kfold = KFold(n_splits=3,shuffle=True,random_state=0)\n",
    "ml.log = {}\n",
    "error, accuracy_train, accuracy_test,_ = ml.train_wCrossValidation(model,nn.BCELoss(),\n",
    "                                            optimizer,train_data,kfold,epochs=100,bat_size=32)\n",
    "ml.save_logging(datetime.now().strftime(\"_%m%d_%H%M%S\"), title='baseline_cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(accuracy_test)[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds,res   = get_odds_res(data.loc[testmatches])\n",
    "test_data  = BettingDataset(odds,res)\n",
    "testloader = DataLoader(test_data,32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ml)\n",
    "acc_test, cm = ml.test_model(model,testloader)\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.dispConfusionMatrix(cm,'Baseline MLP 1x3 test confusion matrix',\n",
    "                        filename='confmat_base_mlp1x3_test',save=True,size=(6,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('tfg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "288ff6fe8157f43ba3e1fb4cfa0011490e9beb907b54ad0d71ae70a61946bdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
